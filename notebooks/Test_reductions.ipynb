{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6839f10-b622-4090-9d6b-cda2a59c3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "import weakref\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "import xarray as xr\n",
    "from xarray import DataArray\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pytest\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy.testing import assert_almost_equal, assert_allclose\n",
    "import os\n",
    "from climate_index_collection.reductions import (\n",
    "    grouped_mean_weighted,\n",
    "    monthly_mean_weighted,\n",
    "    monthly_mean_unweighted,\n",
    "    monthly_anomalies_unweighted,\n",
    "    monthly_anomalies_weighted,\n",
    ")\n",
    "from climate_index_collection.data_loading import load_data_set, VARNAME_MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "886419e7-7179-4bef-b70b-da152615479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========\n",
    "# CREATE TEST DATA PARAMETERS AND FUNCTIONS\n",
    "# ========\n",
    "\n",
    "lon = np.array([120,140,150])\n",
    "lat = np.array([-10, -5, 0])\n",
    "\n",
    "def create_data_array(values,  dim, dim_name) :\n",
    "    \"\"\"\n",
    "    This function creates test DataArrays from given lat, lon, group and groupname and weights.\n",
    "    -----\n",
    "    Parameters:\n",
    "        lat: numpy.adarray, list\n",
    "        lon: numpy.adarray, list\n",
    "        values: numpy.adarray\n",
    "        group: numpy.adarray, list\n",
    "        groupname: str\n",
    "    \"\"\"\n",
    "    # create dummy dataset \n",
    "    data = DataArray(values, \n",
    "                     dims=(dim_name, \n",
    "                           \"lat\", \n",
    "                           \"lon\"), \n",
    "                     coords={dim_name : dim, \n",
    "                             \"lat\": lat, \n",
    "                             'lon': lon})\n",
    "    return data\n",
    "\n",
    "def create_weight_array(wei, dim, dim_name) :    \n",
    "    weights = DataArray(wei, \n",
    "                        dims=(dim_name), \n",
    "                        coords={dim_name : dim})\n",
    "    return weights \n",
    "\n",
    "def create_mean_array(mean, group_unique, group_name) :    \n",
    "    weights = DataArray(mean, \n",
    "                 dims=(group_name, \"lat\", \"lon\"), \n",
    "                 coords={group_name : group_unique, \"lat\": lat, 'lon': lon})\n",
    "    return weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2e3cbf-be06-4fb2-8e11-071c89d9938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# First test DataArray\n",
    "weights_1 = [1, 2, 3, 2]\n",
    "dim_1 = ['a','b','a','c']\n",
    "dim_name_1 = group_name_1 = \"group\" \n",
    "group_unique_1 = np.unique(dim_1)\n",
    "\n",
    "np.random.seed(100)\n",
    "values_1 = np.random.randint(0,2, (len(dim_1), len(lat), len(lon)) ).astype(float)\n",
    "values_1[0,0,0] = np.nan\n",
    "data_1 = create_data_array(values = values_1, \n",
    "                            dim = dim_1,\n",
    "                            dim_name = dim_name_1)\n",
    "weights_1 = create_weight_array(wei = weights_1,\n",
    "                                dim = dim_1,\n",
    "                                dim_name = dim_name_1)\n",
    "\n",
    "\n",
    "# Should be the correct values\n",
    "weighted_mean_1 = np.array(\n",
    "      [[[0.  , 0.75, 0.25],\n",
    "        [1.  , 0.25, 0.25],\n",
    "        [0.  , 0.75, 0.75]],\n",
    "\n",
    "       [[0.  , 0.  , 1.  ],\n",
    "        [0.  , 0.  , 0.  ],\n",
    "        [0.  , 1.  , 0.  ]],\n",
    "\n",
    "       [[1.  , 0.  , 0.  ],\n",
    "        [1.  , 0.  , 0.  ],\n",
    "        [1.  , 1.  , 1.  ]]])\n",
    "\n",
    "weighted_mean_1 = create_mean_array(mean = weighted_mean_1,\n",
    "                                        group_unique = group_unique_1,\n",
    "                                        group_name = group_name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698a03b1-7944-406c-b2c7-9ba1489eaac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Second test DataArray\n",
    "dim_2 = pd.to_datetime([\"2020-02-13\", \"2021-06-13\", \"2021-08-13\", \"2022-02-13\"])\n",
    "dim_name_2 = \"time\"\n",
    "weights_2 = dim_2.days_in_month\n",
    "weights_2 = create_weight_array(wei = weights_2, \n",
    "                                  dim = dim_2, \n",
    "                                  dim_name=dim_name_2)\n",
    "group_unique_2 = np.unique(dim_2.month)\n",
    "group_name_2 = \"month\"\n",
    "values_2 = np.array(\n",
    "      [[[np.nan,  0., 57.],\n",
    "        [57., 57., 57.],\n",
    "        [ 0.,  0.,  57.]],\n",
    "\n",
    "       [[ 0.,  0., 57.],\n",
    "        [ 0.,  0.,  0.],\n",
    "        [ 0., 57.,  0.]],\n",
    "\n",
    "       [[ 0., 57.,  0.],\n",
    "        [57.,  0.,  0.],\n",
    "        [ 0., 57., 57.]],\n",
    "\n",
    "       [[57.,  0.,  0.],\n",
    "        [57.,  0.,  0.],\n",
    "        [57., 57., 57.]]])\n",
    "\n",
    "\n",
    "data_2 = create_data_array(values = values_2, \n",
    "                           dim = dim_2, \n",
    "                           dim_name=dim_name_2)\n",
    "# also create a dataset\n",
    "dataset = data_2.to_dataset(dim=None, name=\"test\", promote_attrs=False)\n",
    "\n",
    "weighted_mean_2 = np.array(\n",
    "      [[[57.,  0., 29.],\n",
    "        [57., 29., 29.],\n",
    "        [28., 28., 57.]],\n",
    "\n",
    "       [[ 0.,  0., 57.],\n",
    "        [ 0.,  0.,  0.],\n",
    "        [ 0., 57.,  0.]],\n",
    "\n",
    "       [[ 0., 57.,  0.],\n",
    "        [57.,  0.,  0.],\n",
    "        [ 0., 57., 57.]]])\n",
    "weighted_mean_2 = create_mean_array(mean = weighted_mean_2,\n",
    "                                        group_unique=group_unique_2,\n",
    "                                        group_name = group_name_2)\n",
    "unweighted_mean_2 = np.array(\n",
    "      [[[57,  0., 28.5,],\n",
    "        [57, 28.5, 28.5,],\n",
    "        [28.5, 28.5,  57]],\n",
    "\n",
    "       [[ 0.,  0., 57.],\n",
    "        [ 0.,  0.,  0.],\n",
    "        [ 0., 57.,  0.]],\n",
    "\n",
    "       [[ 0., 57.,  0.],\n",
    "        [57.,  0.,  0.],\n",
    "        [ 0., 57., 57.]]])\n",
    "unweighted_mean_2 = create_mean_array(mean = unweighted_mean_2,\n",
    "                                        group_unique=group_unique_2,\n",
    "                                        group_name = group_name_2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8de0dc8-b6e2-494f-a564-b245d9a3c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the monthly anomalies for the data now nans will be included!\n",
    "weighted_anomalies_2 = np.    array(\n",
    "        [[[np.nan,   0.,  28.],\n",
    "            [  0.,  28.,  28.],\n",
    "            [-28., -28.,   0.]],\n",
    "\n",
    "           [[  0.,   0.,   0.],\n",
    "            [  0.,   0.,   0.],\n",
    "            [  0.,   0.,   0.]],\n",
    "\n",
    "           [[  0.,   0.,   0.],\n",
    "            [  0.,   0.,   0.],\n",
    "            [  0.,   0.,   0.]],\n",
    "\n",
    "           [[  0.,   0., -29.],\n",
    "            [  0., -29., -29.],\n",
    "            [ 29.,  29.,   0.]]])\n",
    "weighted_anomalies_2 = create_data_array(values = weighted_anomalies_2, \n",
    "                                  dim = dim_2, \n",
    "                                  dim_name=dim_name_2)\n",
    "\n",
    "unweighted_anomalies_2 = np.array(\n",
    "     [[[np.nan,   0. ,  28.5],\n",
    "        [  0. ,  28.5,  28.5],\n",
    "        [-28.5, -28.5,   0. ]],\n",
    "\n",
    "       [[  0. ,   0. ,   0. ],\n",
    "        [  0. ,   0. ,   0. ],\n",
    "        [  0. ,   0. ,   0. ]],\n",
    "\n",
    "       [[  0. ,   0. ,   0. ],\n",
    "        [  0. ,   0. ,   0. ],\n",
    "        [  0. ,   0. ,   0. ]],\n",
    "\n",
    "       [[  0. ,   0. , -28.5],\n",
    "        [  0. , -28.5, -28.5],\n",
    "        [ 28.5,  28.5,   0. ]]])\n",
    "unweighted_anomalies_2 = create_data_array(values = unweighted_anomalies_2, \n",
    "                                  dim = dim_2, \n",
    "                                  dim_name=dim_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef5990c-6c2e-4840-a30e-3f5d7ec955c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"data, weights, dim     , groupby_dim, should\",[ \n",
    "    (data_1, weights_1, group_name_1 , group_name_1 , weighted_mean_1),\n",
    "    (data_2, weights_2, \"time\" , \"time.month\" , weighted_mean_2),\n",
    "                         ])\n",
    "def test_grouped_mean_weighted(data, weights, dim, groupby_dim, should):\n",
    "    \"\"\"Checks if the groupby weighting function gives proper results.\"\"\"\n",
    "    result = grouped_mean_weighted(dobj=data, weights= weights, dim = dim, groupby_dim= groupby_dim)\n",
    "    assert result.equals(should)\n",
    "\n",
    "@pytest.mark.parametrize(\"data_array, should\",[(data_2, weighted_mean_2)])\n",
    "def test_monthly_mean_weighted(data_array, should):\n",
    "    \"\"\"Checks if the monthly mean weighted function gives proper results.\"\"\"\n",
    "    result = monthly_mean_weighted(dobj=data_array)\n",
    "    assert result.equals(should)\n",
    "\n",
    "@pytest.mark.parametrize(\"data_array, should\",[(data_2, unweighted_mean_2)])\n",
    "def test_monthly_mean_unweighted(data_array, should):\n",
    "    \"\"\"Checks if the monthly mean unweighted function gives proper results.\"\"\"\n",
    "    result = monthly_mean_unweighted(dobj=data_array)\n",
    "    assert result.equals(should)\n",
    "\n",
    "# Tests for anomaly functions\n",
    "# check if the results are correct\n",
    "@pytest.mark.parametrize(\"data_array, should\",[(data_2, weighted_anomalies_2)])\n",
    "def test_monthly_anomalies_weighted(data_array, should):\n",
    "    \"\"\"Checks if the monthly anomalies weighted function gives proper results.\"\"\"\n",
    "    result = monthly_anomalies_weighted(dobj=data_array)\n",
    "    assert result.equals(should)\n",
    "\n",
    "@pytest.mark.parametrize(\"data_array, should\",[(data_2, unweighted_anomalies_2)])\n",
    "def test_monthly_anomalies_unweighted(data_array, should):\n",
    "    \"\"\"Checks if the monthly anomalies unweighted function gives proper results.\"\"\"\n",
    "    result = monthly_anomalies_unweighted(dobj=data_array)\n",
    "    assert result.equals(should)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec9ee6ea-bd57-4ac4-91b7-110ed14f58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS FOR THE ANOMALIE FUNCTIONS\n",
    "# Check if the mean of the anomalies is close to 0 for each dataset\n",
    "def variable_close_to_desired(original, actual, desired = 0, relative_tolerance = 1e-5):\n",
    "    \"\"\"\n",
    "    Checks if all spatial point from the input array \"actual\" are close to the desired value.\n",
    "    This assert is based on the numpy.testing.assert_almost_equal.\n",
    "    The decimal used for this is obtained based on the relative tolerance given.\n",
    "    It is calculated as described below\n",
    "        1. absolute accuracy = (maximum - minimum) * relative tolerance.\n",
    "        2. decimal = -1 * Order(absolute accuracy)\n",
    "    Maximum and minimum are derived from the \"original\" DataArray \n",
    "    which was used for the calculation of the DataArray \"actual\".\n",
    "    For some purpose it might make sense to hand original and actual the same DataArray.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    original: xarray.DataArray\n",
    "        DataArray containing field from which e.g. the anomalies were calulated.\n",
    "    actual: xarray.DataArray\n",
    "        Dataset with same variables as orignial but containing the values, \n",
    "        which shall be close to the desired value.\n",
    "        For instance the monthly anomalies derived from original\n",
    "    desired: float\n",
    "        Desired value.\n",
    "        Default to 0.\n",
    "    relative_tolerance : float\n",
    "        Relative tolerance which shall be used to derive the decimal accuracy.\n",
    "        Default to 1e-5.\n",
    "    \n",
    "    \"\"\"\n",
    "    min_value = original.min().values\n",
    "    max_value = original.max().values\n",
    "    absolute_tolerance = (max_value-min_value)*relative_tolerance\n",
    "    # calculate the desired decimal accuracy as \n",
    "    # -1 * Order(absolute accuracy)\n",
    "    decimal = -1 * math.floor(math.log(absolute_tolerance, 10))\n",
    "    assert_almost_equal(\n",
    "            actual = actual.values,\n",
    "            desired = desired,\n",
    "            decimal=decimal)\n",
    "\n",
    "def all_variables_close_to_desired(original, actual, desired = 0, relative_tolerance = 1e-5):\n",
    "    \"\"\"\n",
    "    Checks if all variable from the input DataSet \"actual\" are close to the desired value.\n",
    "    This assert is based on the numpy.testing.assert_almost_equal.\n",
    "    The decimal used for this is obtained based on the relative tolerance given.\n",
    "    It is calculated as described below\n",
    "        1. absolute accuracy = (maximum - minimum) * relative tolerance.\n",
    "        2. decimal = -1 * Order(absolute accuracy)\n",
    "    Maximum and minimum are derived from the \"original\" DataSet for each variable independently. \n",
    "    The \"original\" should be the DataSet from which \"actual\" was calculated.\n",
    "    For some purpose it might make sense to hand original and actual the same DataArray.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    original: xarray.DataSet\n",
    "        Dataset containing field from which e.g. the anomalies were calulated.\n",
    "    actual: xarray.DataSet\n",
    "        Dataset with same variables as orignial but containing the values, \n",
    "        which shall be close to the desired value.\n",
    "        For instance the monthly anomalies derived from original\n",
    "    desired: float\n",
    "        Desired value.\n",
    "        Default to 0.\n",
    "    relative_tolerance : float\n",
    "        Relative tolerance which shall be used to derive the decimal accuracy \n",
    "        for each variable individually.\n",
    "        Default to 1e-5.\n",
    "    \n",
    "    \"\"\"\n",
    "    for variable in original.keys() :\n",
    "        if \"time\" in variable:\n",
    "            continue\n",
    "        variable_close_to_desired(original[variable], actual[variable], desired = 0, relative_tolerance = 1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b255d1ec-fe27-447b-832d-648d823f65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"source_name\", list(VARNAME_MAPPING.keys())+[dataset])\n",
    "def test_monthly_anomalies_weighted_zeromean(source_name, relative_tolerance = 1e-5):\n",
    "    \"\"\"Checks if the mean of the monthly anomalies weighted are all close to 0 using\n",
    "    all_variables_close_to_desired which is based on numpy.testing.assert_almost_equal.\n",
    "    The test will be performed for each spatial gridpoint individually.\n",
    "    \n",
    "    NOTE: \n",
    "        As this is a weighted mean, the values of the anomalies will not sum up to zero with a convenient .mean(\"time\").\n",
    "        One should better check if the monthly_mean_weighted of the anomalies sums up to zero for each month.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    source_name: str\n",
    "        Test dataset name.\n",
    "    relative_tolerance : float\n",
    "        Relative tolerance which shall be used to derive the decimal accuracy \n",
    "        for each variable individually.\n",
    "        Default to 1e-5.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(source_name, xr.Dataset) :\n",
    "        data_set = source_name\n",
    "    else : \n",
    "        # Load test data\n",
    "        path_file = os.path.abspath('')\n",
    "        TEST_DATA_PATH = Path(path_file).parent / \"data/test_data/\"\n",
    "        # TEST_DATA_PATH = Path(__file__).parent / \"../data/test_data/\"\n",
    "        data_set = load_data_set(data_path=TEST_DATA_PATH, data_source_name=source_name)\n",
    "    \n",
    "    anomalies = monthly_anomalies_weighted(dobj=data_set)\n",
    "    anomalies_mean = monthly_mean_weighted(anomalies)\n",
    "    \n",
    "    # check if the mean of the anomalies is close to 0 with desired decimal accuracy \n",
    "    # derived based of relative tolerance for each variable indepenently \n",
    "    all_variables_close_to_desired(\n",
    "        original = data_set, \n",
    "        actual= anomalies_mean, \n",
    "        desired = 0, \n",
    "        relative_tolerance = relative_tolerance)\n",
    "\n",
    "@pytest.mark.parametrize(\"source_name\", list(VARNAME_MAPPING.keys())+[dataset])\n",
    "def test_monthly_anomalies_unweighted_zeromean(source_name, relative_tolerance = 1e-5):\n",
    "    \"\"\"Checks if the mean of the monthly anomalies unweighted are all close to 0 using\n",
    "    all_variables_close_to_desired which is based on numpy.testing.assert_almost_equal\n",
    "    The test will be performed for each spatial gridpoint individually.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    source_name: str or DataSet\n",
    "        Test dataset name.\n",
    "        Or DataSet directly\n",
    "    relative_tolerance : float\n",
    "        Relative tolerance which shall be used to derive the decimal accuracy \n",
    "        for each variable individually.\n",
    "        Default to 1e-5.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(source_name, xr.Dataset) :\n",
    "        data_set = source_name\n",
    "    else : \n",
    "        # Load test data\n",
    "        path_file = os.path.abspath('')\n",
    "        TEST_DATA_PATH = Path(path_file).parent / \"data/test_data/\"\n",
    "        # TEST_DATA_PATH = Path(__file__).parent / \"../data/test_data/\"\n",
    "        data_set = load_data_set(data_path=TEST_DATA_PATH, data_source_name=source_name)\n",
    "    \n",
    "    anomalies = monthly_anomalies_unweighted(dobj=data_set)\n",
    "    anomalies_mean = anomalies.mean(\"time\")\n",
    "    # check if the mean of the anomalies is close to 0 with desired decimal accuracy \n",
    "    # derived based of relative tolerance for each variable indepenently \n",
    "    all_variables_close_to_desired(\n",
    "        original = data_set, \n",
    "        actual= anomalies_mean, \n",
    "        desired = 0, \n",
    "        relative_tolerance = relative_tolerance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c026cb0-823c-443c-996c-cbd3acb0b774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.9.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /work, configfile: pyproject.toml\n",
      "plugins: anyio-3.5.0\n",
      "collected 12 items\n",
      "\n",
      "tmpvytktqlr.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
      "notebooks/tmpvytktqlr.py::test_monthly_anomalies_weighted_zeromean[FOCI]\n",
      "  /srv/conda/envs/notebook/lib/python3.9/site-packages/cfgrib/xarray_plugin.py:11: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "    if LooseVersion(xr.__version__) <= \"0.17.0\":\n",
      "\n",
      "notebooks/tmpvytktqlr.py::test_monthly_anomalies_weighted_zeromean[FOCI]\n",
      "  /srv/conda/envs/notebook/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "    other = LooseVersion(other)\n",
      "\n",
      "notebooks/tmpvytktqlr.py::test_monthly_anomalies_weighted_zeromean[FOCI]\n",
      "notebooks/tmpvytktqlr.py::test_monthly_anomalies_unweighted_zeromean[FOCI]\n",
      "  /srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/coding/times.py:673: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "    dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "\n",
      "notebooks/tmpvytktqlr.py::test_monthly_anomalies_weighted_zeromean[FOCI]\n",
      "notebooks/tmpvytktqlr.py::test_monthly_anomalies_unweighted_zeromean[FOCI]\n",
      "  /srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/indexing.py:423: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "    return np.asarray(array[self.key], dtype=None)\n",
      "\n",
      "notebooks/tmpvytktqlr.py: 12 warnings\n",
      "  /srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 10 times more chunks\n",
      "    return self.array[key]\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================================= \u001b[32m12 passed\u001b[0m, \u001b[33m\u001b[1m18 warnings\u001b[0m\u001b[33m in 19.74s\u001b[0m\u001b[33m =================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipytest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a9a9f-314f-469e-b445-17a352c38cc7",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846832e-84e0-499e-b062-b22ab412875c",
   "metadata": {},
   "source": [
    "rtol = 1e-6\n",
    "result = monthly_anomalies_unweighted(dobj=(FOCI_DATA[\"sea-surface-temperature\"]))\n",
    "# to calculate the absolute tolerance, we use :\n",
    "# ( max(data_set) - min(data_set) ) * rtol\n",
    "min_value = (FOCI_DATA[\"sea-surface-temperature\"]).min().values\n",
    "max_value = (FOCI_DATA[\"sea-surface-temperature\"]).max().values\n",
    "atol = (  max_value - min_value ) * rtol\n",
    "print(min_value, max_value, atol, rtol)\n",
    "result_mean = result.mean(\"time\")\n",
    "# check if the results are equall to zero with an accuracy\n",
    "print(np.allclose(result_mean*0, result_mean,\n",
    "                rtol = rtol,\n",
    "                atol = atol))\n",
    "plt.imshow(np.isclose(result_mean*0, result_mean,\n",
    "                rtol = rtol,\n",
    "                atol = atol))\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.imshow(result_mean, cmap = \"RdBu_r\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb947c7b-63f0-4931-a25e-49cd07698930",
   "metadata": {},
   "source": [
    "rtol = 1e-3\n",
    "data_set = dataset[\"test\"]\n",
    "\"\"\"Checks if the mean of the anomalies are all close to 0 using numpy.allclose()\n",
    "rtol : float \n",
    "    relative accuracy Default of 1e-6.\n",
    "\n",
    "Absolute accuracy is calculated with (max(data_set) - min(data_set)) * rtol\n",
    "For further information look at numpy.allclose()\n",
    "From numpy:\n",
    "\"The tolerance values are positive, typically very small numbers. \n",
    "The relative difference (rtol * abs(b)) and the absolute difference atol are added together \n",
    "to compare against the absolute difference between a and b.\"\n",
    "\"\"\"\n",
    "result = monthly_anomalies_weighted(dobj=data_set)\n",
    "# to calculate the absolute tolerance, we use :\n",
    "# ( max(data_set) - min(data_set) ) * rtol\n",
    "min_value = data_set.min().values\n",
    "max_value = data_set.max().values\n",
    "atol = (max_value-min_value)*rtol\n",
    "print(rtol, atol)\n",
    "result_mean = result.mean(\"time\")\n",
    "plt.figure()\n",
    "plt.imshow(result_mean)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.imshow(np.isclose(result_mean*0, result_mean,\n",
    "            rtol = rtol,\n",
    "            atol = atol,\n",
    "            equal_nan = True))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb68dcf-805e-425e-ac8c-d22b4bc82362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
